{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcb_envs import MultiPathGridEnv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from dqn_agent import DQNAgent\n",
    "from agent_trainer import QLearner\n",
    "from IPython.display import display, clear_output\n",
    "import csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_agents = 1\n",
    "grid_size = [15,10]\n",
    "obstacles = [(3,3),(6,2), (6,3), (9,4), (9,3), (10,4), (8,9), (8,8)]\n",
    "starts = [(13,8)]\n",
    "goals = [(1,1)] # orig: (2,1) \n",
    "to_train = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env1 = MultiPathGridEnv(obstacles, starts, goals, grid_size=grid_size, obs_size=[40,40], agents_n=n_agents, train=to_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env1.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = grid_size[0]*grid_size[1]\n",
    "action_dim = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent = DQNAgent(action_dim, device)\n",
    "reward_sums_list = []\n",
    "epsilon_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent):\n",
    "    N = 100000\n",
    "    max_time_steps = 3000\n",
    "    epsilon = 0.4\n",
    "    decay = 0.9999\n",
    "    min_epsilon = 0.1\n",
    "    for episode in range(N):\n",
    "        reward_sum = 0\n",
    "        state = env1.reset()[0] # Indent into 0 for now and elsewhere :-) \n",
    "        epsilon = max(min_epsilon, epsilon*decay)\n",
    "        epsilon_history.append(epsilon)\n",
    "        for i in range(max_time_steps):\n",
    "            chosen_actions = []\n",
    "            t_state = (torch.from_numpy(state[0]).reshape(1,1,grid_size[0],grid_size[1]).float().to(device),torch.Tensor([state[1]]).to(device))\n",
    "            action = agent.epsilon_greedy_action(t_state, epsilon)\n",
    "            if type(action) is not list:\n",
    "                action = [action]\n",
    "            next_state, reward, terminal = env1.step(action)\n",
    "            next_state = next_state[0]\n",
    "            buff_next_state = (next_state[0].reshape(1,grid_size[0],grid_size[1]), next_state[1])\n",
    "            buff_state = (state[0].reshape(1,grid_size[0],grid_size[1]), state[1])\n",
    "            reward_sum += reward[0]\n",
    "            agent.memory.add_to_buffer((buff_state, buff_next_state, action, [reward], [terminal]))\n",
    "            state = next_state\n",
    "            #env1.render()\n",
    "            if terminal:\n",
    "                #clear_output(wait=False)\n",
    "                #display('Current episode: ' + str(episode))\n",
    "                reward_sums_list.append(reward_sum)\n",
    "                reward_sum = 0\n",
    "                break\n",
    "        if episode !=0:\n",
    "            agent.update(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f78bdeac98c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdqn_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-7462c62ca2c8>\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(agent)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/DataScience_Bath_cws/Thesis_dev/marl_board_env/dqn_agent.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, update_rate)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mterminals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterminals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_Q_Network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/DataScience_Bath_cws/Thesis_dev/marl_board_env/dqn_agent.py\u001b[0m in \u001b[0;36mupdate_Q_Network\u001b[0;34m(self, state, next_state, action, reward, terminal)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m#self.network_loss_history.append(q_network_loss.double()) <-- PROBLEM: GPU leakage!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnet_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mq_network_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnet_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_agent(dqn_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it too... \n",
    "wtr = csv.writer(open ('learning_history.csv', 'w'), delimiter=',', lineterminator='\\n')\n",
    "for x in reward_sums_list : wtr.writerow ([x]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(8)\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('no. of episodes')\n",
    "ax1.set_ylabel('Total reward', color=color)\n",
    "cumsum_vec = np.cumsum(np.insert(reward_sums_list, 0, 0)) \n",
    "ma_vec = (cumsum_vec[100:] - cumsum_vec[:-100]) / 100 # <<-- 100 instead of 5\n",
    "ax1.plot(range(0,len(ma_vec)), ma_vec, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('epsilon value', color=color)\n",
    "cumsum_vec_e = np.cumsum(np.insert(epsilon_history, 0, 0)) \n",
    "ma_vec_e = (cumsum_vec_e[100:] - cumsum_vec_e[:-100]) / 100\n",
    "ax2.plot(range(0,len(ma_vec_e)), ma_vec_e, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max reward sum received: \", np.max(reward_sums_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learnt Policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(agent):\n",
    "    max_time_steps = 5000\n",
    "    reward_sum = 0\n",
    "    state = env1.reset()[0]\n",
    "    for i in range(max_time_steps):\n",
    "        t_state = (torch.from_numpy(state[0]).reshape(1,1,grid_size[0],grid_size[1]).float().to(device),torch.Tensor([state[1]]).to(device))\n",
    "        action = agent.policy_action(t_state)\n",
    "        next_state, reward, terminal = env1.step([action])\n",
    "        next_state = next_state[0]\n",
    "        reward_sum += reward[0]\n",
    "        state = next_state\n",
    "        env1.render()\n",
    "        if terminal:\n",
    "            state = env1.reset()[0]\n",
    "            reward_sum = 0\n",
    "            break\n",
    "    print(\"Reward sum is: \", reward_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy(dqn_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpratability test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env1.reset()\n",
    "env1.step([0])\n",
    "state = env1.step([0])[0][0]\n",
    "t_state = (torch.from_numpy(state[0]).reshape(1,1,grid_size[0],grid_size[1]).float().to(device),torch.Tensor([state[1]]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent.qnet(t_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env1.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dqn_agent.qnet.state_dict(), 'models/concat-mlp.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalisation test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env1_mod1 = MultiPathBoardEnv(obstacles, starts, [(7,4)], grid_size=grid_size, agents_n=n_agents) \n",
    "reward_sums_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env1_mod1.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent():\n",
    "    N = 2\n",
    "    max_time_steps = 1000\n",
    "    for episode in range(N):\n",
    "        reward_sum = 0\n",
    "        state = env1_mod1.reset().flatten()\n",
    "        for i in range(max_time_steps):\n",
    "            chosen_actions = []\n",
    "            action = dqn_agent.policy_action(torch.from_numpy(state.flatten()).float())\n",
    "            next_state, reward, terminal = env1_mod1.step([action])\n",
    "            next_state = next_state.flatten() \n",
    "            reward_sum += reward[0]\n",
    "            state = next_state\n",
    "            env1_mod1.render()\n",
    "            if terminal:\n",
    "                time.sleep(0.2)\n",
    "                reward_sums_list.append(reward_sum)\n",
    "                state = env1_mod1.reset().flatten()\n",
    "                reward_sum = 0\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERALISATION TEST TO BE COMPLETED!! :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01316321])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(0, 0.1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
