{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcb_envs import MultiPathGridEnv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from drqn_agent import DRQNAgent\n",
    "from agent_trainer import QLearner\n",
    "from IPython.display import display, clear_output\n",
    "from IPython.core.debugger import set_trace \n",
    "import csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_agents = 1\n",
    "grid_size = [15,10]\n",
    "obs_size = [40,40] # Basically get full grid :-) [for now!]\n",
    "obstacles = [(3,3),(6,2), (6,3), (9,4), (9,3), (10,4), (8,9), (8,8)]\n",
    "starts = [(13,8)]\n",
    "goals = [(1,1)] # orig: (2,1) \n",
    "to_train = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env1 = MultiPathGridEnv(obstacles, starts, goals, grid_size=grid_size, obs_size=obs_size, agents_n=n_agents, train=to_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env1.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = grid_size[0]*grid_size[1]\n",
    "action_dim = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "drqn_agent = DRQNAgent(action_dim, device)\n",
    "reward_sums_list = []\n",
    "epsilon_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent):\n",
    "    N = 100000\n",
    "    max_time_steps = 3000\n",
    "    epsilon = 0.4\n",
    "    decay = 0.9999\n",
    "    min_epsilon = 0.1\n",
    "    context = (Variable(torch.zeros(3, 1, 181).float()), Variable(torch.zeros(3, 1, 181).float()))\n",
    "    for episode in range(N):\n",
    "        reward_sum = 0\n",
    "        episode_experience = []\n",
    "        state = env1.reset()[0] # Indent into 0 for now and elsewhere :-) \n",
    "        epsilon = max(min_epsilon, epsilon*decay)\n",
    "        epsilon_history.append(epsilon)\n",
    "        for i in range(max_time_steps):\n",
    "            chosen_actions = []\n",
    "            t_state = (torch.from_numpy(state[0]).reshape(1,1,grid_size[0],grid_size[1]).float().to(device),torch.Tensor([state[1]]).to(device))\n",
    "            # TO DO: Insert context/hidden state from previous timestep action selection??...\n",
    "            action, context = agent.epsilon_greedy_action(t_state, context, epsilon)\n",
    "            if type(action) is not list:\n",
    "                action = [action]\n",
    "            next_state, reward, terminal = env1.step(action)\n",
    "            next_state = next_state[0]\n",
    "            buff_state = (state[0].reshape(1,grid_size[0],grid_size[1]), state[1])\n",
    "            obs = (buff_state, action, [reward])\n",
    "            episode_experience.append(obs)\n",
    "            reward_sum += reward[0]\n",
    "            state = next_state\n",
    "            #env1.render()\n",
    "            if terminal:\n",
    "                reward_sums_list.append(reward_sum)\n",
    "                reward_sum = 0\n",
    "                agent.memory.add_to_buffer(tuple(episode_experience))\n",
    "                break\n",
    "        if episode !=0:\n",
    "            agent.update(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode no. 0 replay is: (((array([[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0., -10.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0., -10.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., -10.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., -10.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  13.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,  16.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0., -10.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0., -10.,   0.,   0.,   0.,   0.]]]), 2.8284271247461903), [0], [[-1.7771241507177988]]), ((array([[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0., -10.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0., -10.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., -10.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., -10.,   0.],\n",
      "        [  0.,   0.,   0.,  13.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  10.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,  16.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0., -10.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0., -10.,   0.,   0.,   0.,   0.]]]), 3.605551275463989), [4], [[-10.2228758492822]]))\n",
      "Episode no. 1 replay is: (((array([[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0., -10.,   0.,  16.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  13.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., -10.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0., -10.,   0.,   0.,   0.,   0.,   0.,   0., -10.,   0.],\n",
      "        [  0.,   0., -10.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0., -10.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0., -10.,   0.,   0.,   0.]]]), 3.605551275463989), [0], [[-0.5567263847043904]]), ((array([[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0., -10.,   0.,  16.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  13.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  10.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., -10.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0., -10.,   0.,   0.,   0.,   0.,   0.,   0., -10.,   0.],\n",
      "        [  0.,   0., -10.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0., -10.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0., -10.,   0.,   0.,   0.]]]), 3.1622776601683795), [0], [[-0.8377223398316205]]), ((array([[[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0., -10.,   0.,  16.,   0.,   0.,  13.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  10.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  10.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., -10.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0., -10.,   0.,   0.,   0.,   0.,   0.,   0., -10.,   0.],\n",
      "        [  0.,   0., -10.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0., -10.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0., -10.,   0.,   0.,   0.]]]), 3.0), [7], [[-0.2360679774997898]]), ((array([[[  0.,   0.,   0.,   0.,   0.,   0.,  13.,   0.,   0.,   0.],\n",
      "        [  0.,   0., -10.,   0.,  16.,   0.,   0.,  10.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  10.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  10.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., -10.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0., -10.,   0.,   0.,   0.,   0.,   0.,   0., -10.,   0.],\n",
      "        [  0.,   0., -10.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0., -10.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0., -10.,   0.,   0.,   0.]]]), 2.23606797749979), [0], [[-11.5923591472464]]))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "gather() received an invalid combination of arguments - got (tuple, index=Tensor, dim=int), but expected one of:\n * (Tensor input, name dim, Tensor index, bool sparse_grad, Tensor out)\n * (Tensor input, int dim, Tensor index, bool sparse_grad, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-20aa5356a659>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrqn_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-3a17715de193>\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(agent)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/DataScience_Bath_cws/Thesis_dev/marl_board_env/drqn_agent.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, update_rate)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# May need to give it a good squeeze??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_Q_Network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/DataScience_Bath_cws/Thesis_dev/marl_board_env/drqn_agent.py\u001b[0m in \u001b[0;36mupdate_Q_Network\u001b[0;34m(self, state, action, reward)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# THIS UPDATE WILL BE TOTALLY DIFFERENT... Context/Hidden?! where will that come from...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m181\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m181\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mqsa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mqsa_next_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mqsa_next_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqsa_next_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: gather() received an invalid combination of arguments - got (tuple, index=Tensor, dim=int), but expected one of:\n * (Tensor input, name dim, Tensor index, bool sparse_grad, Tensor out)\n * (Tensor input, int dim, Tensor index, bool sparse_grad, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "train_agent(drqn_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
