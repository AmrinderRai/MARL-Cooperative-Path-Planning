{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcb_envs import MultiPathGridEnv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_agents = 1\n",
    "grid_size = [15,10]\n",
    "obstacles = [(3,3),(6,2), (6,3), (9,4), (9,3), (10,4), (8,9), (8,8)]\n",
    "starts = [(13,8)]\n",
    "goals = [(1,1)] # orig: (2,1) \n",
    "to_train = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env1 = MultiPathGridEnv(obstacles, starts, goals, grid_size=grid_size, agents_n=n_agents, train=to_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAJCCAYAAADTM/ATAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAATCklEQVR4nO3df6jleX3f8de7O5FkTYKWvWk7u0vHFLEVaTFcgokQQSNsGtH+0T+UGmwq7D9NYkKK1QZ6uP8VGtIEGlIGNQayrH9sLNVgEyW/pGCXXFdTV9c0YrbrOJvuFWkS4h9mybt/zE3YnZ2dee89Z+73e2ceD1jmnu85c75vvnNmn/M5P76nujsAcCN/a+kBADgbBAOAEcEAYEQwABgRDABGBAOAkTMXjKq6r6r+sKq+VFXvWXqeNamqe6vqd6rqsar6fFW9a+mZ1qiq7qiqz1TVry89yxpV1Uuq6qGq+uLxY+n7lp5pTarqp47/fj1aVQ9W1bcuPdNpOVPBqKo7kvxikh9K8sokb6uqVy471ao8neSnu/sfJXlNkn/t+FzTu5I8tvQQK/YLSX6ju/9hkn8Sx+pvVNXdSX4iyX53vyrJHUneuuxUp+dMBSPJ9yb5Und/ubu/meRDSd6y8Eyr0d1Pdvcjxz//ea78Rb972anWparuSfLDSd639CxrVFXfmeQHkrw/Sbr7m939/5adanXOJfm2qjqX5M4klxee59SctWDcneQrz7h8Kf6HeE1VdSHJq5M8vOwkq/PzSd6d5K+WHmSlvjvJUZJfPn7a7n1V9eKlh1qL7v5qkp9N8kSSJ5P8aXd/fNmpTs9ZC0ZdY5tzm1ylqr49ya8l+cnu/rOl51mLqnpTkqe6+9NLz7Ji55J8T5Jf6u5XJ/mLJF4rPFZVL82VZzVeluR8khdX1duXner0nLVgXEpy7zMu35PbaDk4UVXfkiuxeKC7P7z0PCvz2iRvrqrHc+XpzNdX1a8uO9LqXEpyqbv/emX6UK4EhCt+MMkfd/dRd/9lkg8n+f6FZzo1Zy0Yv5/k5VX1sqp6Ua682PSRhWdajaqqXHnu+bHu/rml51mb7n5vd9/T3Rdy5bHz29192/zrcKK7/yTJV6rqFceb3pDkCwuOtDZPJHlNVd15/PftDbmN3hRwbukBXojufrqqfizJb+bKuxM+0N2fX3isNXltkh9J8rmq+uzxtn/X3R9bcCbOnh9P8sDxP8q+nORHF55nNbr74ap6KMkjufKuxM8kubjsVKennN4cgImz9pQUAAsRDABGBAOAEcEAYEQwABg5k8GoqvuXnmHtHKPrc3xuzDG6vtvx+JzJYCS57f6gTsAxuj7H58Yco+u77Y7PWQ0GAKfsVD+4d9ddd/WFCxe2vp+jo6Ps7e1tP9AKXb68m1NjfeMb38idd9659f2cP39+B9Osz638GNoVx+j6btXj8/jjj+drX/vatU70erqnBrlw4UIODw9Pc5dnzsHBwdIjPMtms1l6BOAU7e/vP+91npICYEQwABgRDABGBAOAEcEAYEQwABgRDABGBAOAEcEAYEQwABgRDABGBAOAka2CUVX3VdUfVtWXquo9uxoKgPU5cTCq6o4kv5jkh5K8MsnbquqVuxoMgHXZZoXxvUm+1N1f7u5vJvlQkrfsZiwA1mabYNyd5CvPuHzpeBsAt6BtgnGtb2R6ztf3VdX9VXVYVYdHR0db7A6AJW0TjEtJ7n3G5XuSPOf7Rbv7Ynfvd/f+rfh1hgC3i22C8ftJXl5VL6uqFyV5a5KP7GYsANbmxN/p3d1PV9WPJfnNJHck+UB3f35nkwGwKicORpJ098eSfGxHswCwYj7pDcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjAgGACNbnXyQ3dtsNkuPsGoHBwdLj/Asa/zzWtsxWps1/pmdFVYYAIwIBgAjggHAiGAAMCIYAIwIBgAjggHAiGAAMCIYAIwIBgAjggHAiGAAMCIYAIwIBgAjggHAiGAAMCIYAIwIBgAjggHAiGAAMCIYAIwIBgAjggHAiGAAMCIYAIwIBgAjggHAiGAAMCIYAIwIBgAjggHAiGAAMCIYAIwIBgAjggHAyLmlB1jSwcHB0iM8x2azWXoEzjiPIW4WKwwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGDkxMGoqnur6neq6rGq+nxVvWuXgwGwLtt8gdLTSX66ux+pqu9I8umq+kR3f2FHswGwIideYXT3k939yPHPf57ksSR372owANZlJ69hVNWFJK9O8vA1rru/qg6r6vDo6GgXuwNgAVsHo6q+PcmvJfnJ7v6zq6/v7ovdvd/d+3t7e9vuDoCFbBWMqvqWXInFA9394d2MBMAabfMuqUry/iSPdffP7W4kANZomxXGa5P8SJLXV9Vnj//7pzuaC4CVOfHbarv7fySpHc4CwIr5pDcAI4IBwIhgADAiGACMCAYAI4IBwIhgADAiGACMCAYAI4IBwIhgADAiGACMVHef2s729/f78PDw1PZ3Fh0cHCw9wrNsNpulRwBO0f7+fg4PD695YlkrDABGBAOAEcEAYEQwABgRDABGBAOAEcEAYEQwABgRDABGBAOAEcEAYEQwABgRDABGBAOAEcEAYEQwABgRDABGBAOAEcEAYEQwABgRDABGBAOAEcEAYEQwABgRDABGBAOAEcEAYEQwABgRDABGBAOAEcEAYEQwABgRDABGBAOAEcEAYOTc0gPwbJvNZukReAEODg6WHuE5PIa4WawwABgRDABGBAOAEcEAYEQwABgRDABGBAOAEcEAYEQwABgRDABGBAOAEcEAYEQwABgRDABGtg5GVd1RVZ+pql/fxUAArNMuVhjvSvLYDu4HgBXbKhhVdU+SH07yvt2MA8BabbvC+Pkk707yVzuYBYAVO3EwqupNSZ7q7k/f4Hb3V9VhVR0eHR2ddHcALGybFcZrk7y5qh5P8qEkr6+qX736Rt19sbv3u3t/b29vi90BsKQTB6O739vd93T3hSRvTfLb3f32nU0GwKr4HAYAI+d2cSfd/btJfncX9wXAOllhADAiGACMCAYAI4IBwIhgADAiGACMCAYAI4IBwIhgADAiGACMCAYAI4IBwMhOTj4It6vNZrP0CNwCDg4Olh7hb1y+fPl5r7PCAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgJFzSw8AL8TBwcHSIzzLZrNZegRuAWt6HH30ox993uusMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARrYKRlW9pKoeqqovVtVjVfV9uxoMgHXZ9vswfiHJb3T3P6+qFyW5cwczAbBCJw5GVX1nkh9I8i+TpLu/meSbuxkLgLXZ5imp705ylOSXq+ozVfW+qnrx1Teqqvur6rCqDo+OjrbYHQBL2iYY55J8T5Jf6u5XJ/mLJO+5+kbdfbG797t7f29vb4vdAbCkbYJxKcml7n74+PJDuRIQAG5BJw5Gd/9Jkq9U1SuON70hyRd2MhUAq7Ptu6R+PMkDx++Q+nKSH91+JADWaKtgdPdnk+zvaBYAVswnvQEYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBk27PVwqnabDZLj8AL9XuHS0/wbK9zvtSTssIAYEQwABgRDABGBAOAEcEAYEQwABgRDABGBAOAEcEAYEQwABgRDABGBAOAEcEAYEQwABgRDABGBAOAEcEAYEQwABgRDABGBAOAEcEAYEQwABgRDABGBAOAEcEAYEQwABgRDABGBAOAEcEAYEQwABgRDABGBAOAEcEAYEQwABgRDABGzi09wJIODg6WHuE5NpvN0iPAbr1uf+kJnu33Dpee4LnWdoyehxUGACOCAcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjAgGACOCAcDIVsGoqp+qqs9X1aNV9WBVfeuuBgNgXU4cjKq6O8lPJNnv7lcluSPJW3c1GADrsu1TUueSfFtVnUtyZ5LL248EwBqdOBjd/dUkP5vkiSRPJvnT7v74rgYDYF22eUrqpUnekuRlSc4neXFVvf0at7u/qg6r6vDo6OjkkwKwqG2ekvrBJH/c3Ufd/ZdJPpzk+6++UXdf7O797t7f29vbYncALGmbYDyR5DVVdWdVVZI3JHlsN2MBsDbbvIbxcJKHkjyS5HPH93VxR3MBsDLntvnN3b1JstnRLACsmE96AzAiGACMCAYAI4IBwIhgADAiGACMCAYAI4IBwIhgADAiGACMCAYAI4IBwMhWJx98oS5fvpyDg4PT3OV1bTbOmwi3ndftLz3BmWWFAcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjAgGACPnTnNn58+fz2azOc1dArAjVhgAjAgGACOCAcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjAgGACM3DEZVfaCqnqqqR5+x7W9X1Seq6o+Of33pzR0TgKVNVhgfTHLfVdvek+S3uvvlSX7r+DIAt7AbBqO7P5nk61dtfkuSXzn++VeS/LMdzwXAypz0NYy/091PJsnxr9/1fDesqvur6rCqDo+Ojk64OwCWdtNf9O7ui9293937e3t7N3t3ANwkJw3G/62qv5ckx78+tbuRAFijkwbjI0necfzzO5L8t92MA8BaTd5W+2CSTyV5RVVdqqp3JvkPSd5YVX+U5I3HlwG4hZ270Q26+23Pc9UbdjwLACvmk94AjAgGACOCAcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjAgGACOCAcCIYAAwcsOTD+7S5cuXc3BwcJq7PHM2m83SI/ACrPHx7DHEzWKFAcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjJw7zZ2dP38+m83mNHfJLebg4GDpEZ7F45nbiRUGACOCAcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjAgGACOCAcCIYAAwIhgAjAgGACOCAcDIDYNRVR+oqqeq6tFnbPuPVfXFqvpfVfVfq+olN3dMAJY2WWF8MMl9V237RJJXdfc/TvK/k7x3x3MBsDI3DEZ3fzLJ16/a9vHufvr44v9Mcs9NmA2AFdnFaxj/Ksl/38H9ALBiWwWjqn4mydNJHrjObe6vqsOqOjw6OtpmdwAs6MTBqKp3JHlTkn/R3f18t+vui9293937e3t7J90dAAs7d5LfVFX3Jfm3SV7X3d/Y7UgArNHkbbUPJvlUkldU1aWqemeS/5zkO5J8oqo+W1X/5SbPCcDCbrjC6O63XWPz+2/CLACsmE96AzAiGACMCAYAI4IBwIhgADAiGACMCAYAI4IBwIhgADAiGACMCAYAI4IBwMiJTm8OS9lsNkuPALctKwwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGDkhsGoqg9U1VNV9eg1rvs3VdVVddfNGQ+AtZisMD6Y5L6rN1bVvUnemOSJHc8EwArdMBjd/ckkX7/GVf8pybuT9K6HAmB9TvQaRlW9OclXu/sPBre9v6oOq+rw6OjoJLsDYAVecDCq6s4kP5Pk309u390Xu3u/u/f39vZe6O4AWImTrDD+QZKXJfmDqno8yT1JHqmqv7vLwQBYl3Mv9Dd09+eSfNdfXz6Oxn53f22HcwGwMpO31T6Y5FNJXlFVl6rqnTd/LADW5oYrjO5+2w2uv7CzaQBYLZ/0BmBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgBHBAGBEMAAYEQwARgQDgJHqPr1vWK2qoyT/Zwd3dVcSp1O/Psfo+hyfG3OMru9WPT5/v7uv+W13pxqMXamqw+7eX3qONXOMrs/xuTHH6Ppux+PjKSkARgQDgJGzGoyLSw9wBjhG1+f43JhjdH233fE5k69hAHD6zuoKA4BTJhgAjAgGACOCAcCIYAAw8v8BsDkaH0EleugAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env1.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env 1 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reply Memory\n",
    "Batches of transitions are decorrelated (due to random sampling from past experience). \n",
    "Stabilizes and improved the DQN training procedure!\n",
    "\n",
    "-- Note: We may not ever use this explicitly defined object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer():\n",
    "    def __init__(self, maxlen):\n",
    "        self.buffer = deque(maxlen=maxlen)\n",
    "        self.priorities = deque(maxlen=maxlen)\n",
    "        \n",
    "    def add_to_buffer(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.append(max(self.priorities, default=1))\n",
    "        \n",
    "    def get_probabilities(self, priority_scale):\n",
    "        scaled_priorities = np.array(self.priorities) ** priority_scale\n",
    "        sample_probabilities = scaled_priorities / sum(scaled_priorities)\n",
    "        return sample_probabilities\n",
    "    \n",
    "    def get_importance(self, probabilities): # not really needed :-) \n",
    "        importance = 1/len(self.buffer) * 1/probabilities\n",
    "        importance_normalized = importance / max(importance)\n",
    "        return importance_normalized\n",
    "        \n",
    "    def sample_minibatch(self, batch_size, priority_scale=1.0):\n",
    "        sample_size = min(len(self.buffer), batch_size)\n",
    "        sample_probs = self.get_probabilities(priority_scale)\n",
    "        sample_indices = random.choices(range(len(self.buffer)), k=sample_size, weights=sample_probs)\n",
    "        buffer_list = list(self.buffer)\n",
    "        samples = [buffer_list[i] for i in sample_indices]\n",
    "        #samples = np.array(buffer_list)[sample_indices]\n",
    "        #importance = self.get_importance(sample_probs[sample_indices])\n",
    "        return map(list, zip(*samples)) # < TO DO: Map to Tensors instead of list??? \n",
    "    \n",
    "    def set_priorities(self, indices, errors, offset=0.1):\n",
    "        for i,e in zip(indices, errors):\n",
    "            self.priorities[i] = abs(e) + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self):\n",
    "        self.buffer = []\n",
    "        self.priorities = []\n",
    "\n",
    "    def add_to_buffer(self, data):\n",
    "        self.buffer.append(data)\n",
    "        self.priorities.append(max(self.priorities, default=1))\n",
    "        \n",
    "    def get_probabilities(self, priority_scale):\n",
    "        scaled_priorities = np.array(self.priorities) **priority_scale\n",
    "        sample_probabilities = scaled_priorities / sum(scaled_priorities)\n",
    "\n",
    "        return sample_probabilities \n",
    "        \n",
    "    def sample_minibatch(self,minibatch_length):\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        terminals = []\n",
    "        for i in range(minibatch_length):\n",
    "            random_int = np.random.randint(0, len(self.buffer)-1) \n",
    "            transition = self.buffer[random_int]\n",
    "            states.append(transition[0])\n",
    "            next_states.append(transition[1])\n",
    "            actions.append(transition[2])\n",
    "            rewards.append(transition[3])\n",
    "            terminals.append(transition[4])\n",
    "        return torch.Tensor(states), torch.Tensor(next_states), torch.Tensor(actions), torch.Tensor(rewards), torch.Tensor(terminals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_ConvNet(nn.Module):\n",
    "    \"\"\"For environments of 2d image envs \"\"\"\n",
    "    def __init__(self, action_dim):\n",
    "        super(Q_ConvNet, self).__init__()\n",
    "        self.number_of_actions = action_dim\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(1, 6, 5, stride=2, padding=1) # Is padding required?\n",
    "        self.conv2 = torch.nn.Conv2d(6, 12, 3, stride=1, padding=1) # Is padding required?\n",
    "        \n",
    "        self.fcv1 = nn.Linear(336, 248) # Value stream... \n",
    "        self.fcv2 = nn.Linear(248, 1)\n",
    "        \n",
    "        self.fca1 = nn.Linear(336, 248)\n",
    "        self.fcv2 = nn.Linear(248, self.number_of_actions)\n",
    "        \n",
    "\n",
    "    def forward(self, state):\n",
    "        state = F.relu(self.conv1(state))\n",
    "        state = F.relu(self.conv2(state))\n",
    "        state = state.view(state.size(0), -1) \n",
    "        \n",
    "        v = F.relu(self.fcv1(state)) # Value stream... \n",
    "        v = self.fcv2(state)\n",
    "        \n",
    "        a = F.relu(self.fca1(state)) # Advantage stream... \n",
    "        a = self.fca2(state)\n",
    "        \n",
    "        q = v + a-a.mean()\n",
    "        \n",
    "        return q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = grid_size[0]*grid_size[1]\n",
    "action_dim = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.qnet = Q_ConvNet(action_dim)\n",
    "        self.qnet_optim = torch.optim.Adam(self.qnet.parameters(), lr=0.001) # Optimiser to optimize our qnet parameters with learning rate 0.01\n",
    "        self.discount_factor = 0.99 \n",
    "        self.MSELoss_function = nn.MSELoss() # Takes input (required_grad=True) and output, then we can backprop with respect to this cost function to calcualte loss :) \n",
    "        self.memory = PrioritizedReplayBuffer(1000)\n",
    "        self.network_loss_history = []\n",
    "        \n",
    "    def epsilon_greedy_action(self, state, epsilon):\n",
    "        if np.random.uniform(0, 1) < epsilon: \n",
    "            return random.sample(env1.action_space,1)[0] \n",
    "        else: \n",
    "            network_output_to_numpy = self.qnet(state).cpu().data.numpy()\n",
    "            max_value = np.max(network_output_to_numpy)\n",
    "            max_indices = np.nonzero(network_output_to_numpy == max_value)[0]\n",
    "            policy_action = np.random.choice(max_indices) \n",
    "            return policy_action  \n",
    "    \n",
    "    def policy_action(self, state):\n",
    "        network_output_to_numpy = self.qnet(state).cpu().data.numpy()\n",
    "        max_value = np.max(network_output_to_numpy)\n",
    "        max_indices = np.nonzero(network_output_to_numpy == max_value)[0]\n",
    "        policy_action = np.random.choice(max_indices) \n",
    "        return policy_action  \n",
    "    \n",
    "    # Update Q Network based on our observation: \n",
    "    # TIP: Look at the Bellman equation :) \n",
    "    def update_Q_Network(self, state, next_state, action, reward, terminal):\n",
    "        qsa = torch.gather(self.qnet(state), dim=1, index=action.long()) # Q(s,a) (Value for selecting action given current Q-policy)\n",
    "        qsa_next_actions = self.qnet(next_state)\n",
    "        qsa_next_action, _ = torch.max(qsa_next_actions, dim=1, keepdim=True) # AGAIN is this fair???? \n",
    "        not_terminal = 1-terminal # <- If it is not terminal (negation) then we will apply additional discount value: \n",
    "        reward = reward.resize_(not_terminal.size()) # <<<-- But what made it out of shape to (batch_size,1,1)\n",
    "        qsa_next_target = reward + not_terminal * self.discount_factor * qsa_next_action\n",
    "        q_network_loss = self.MSELoss_function(qsa, qsa_next_target.detach()) # ** Why detatch qsa_next_target so that no gradients pass it?\n",
    "        self.network_loss_history.append(q_network_loss.double())\n",
    "        self.qnet_optim.zero_grad() # Restard gradients for optimisation \n",
    "        q_network_loss.backward() # <- Calculate gradient... ** But what were we tracking??** \n",
    "        self.qnet_optim.step()  # Take a step according the gradient calculated above??** 🤔\n",
    "        \n",
    "    def update(self, update_rate):\n",
    "        for i in range(update_rate):\n",
    "            states, next_states, actions, rewards, terminals = self.memory.sample_minibatch(120)\n",
    "            states = torch.Tensor(states)\n",
    "            next_states = torch.Tensor(next_states)\n",
    "            actions = torch.Tensor(actions)\n",
    "            rewards = torch.Tensor(rewards)\n",
    "            terminals = torch.Tensor(terminals) \n",
    "            self.update_Q_Network(states, next_states, actions, rewards, terminals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[2,3],[4,3]]).reshape(1,2,2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent = DQNAgent(state_dim, action_dim)\n",
    "reward_sums_list = []\n",
    "epsilon_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent):\n",
    "    N = 100000\n",
    "    max_time_steps = 3000\n",
    "    epsilon = 0.6\n",
    "    decay = 0.9999\n",
    "    min_epsilon = 0.1\n",
    "    for episode in range(N):\n",
    "        reward_sum = 0\n",
    "        state = env1.reset()\n",
    "        epsilon = max(min_epsilon, epsilon*decay)\n",
    "        epsilon_history.append(epsilon)\n",
    "        for i in range(max_time_steps):\n",
    "            chosen_actions = []\n",
    "            action = agent.epsilon_greedy_action(torch.from_numpy(state).reshape(1,1,15,10).float(), epsilon)\n",
    "            if type(action) is not list:\n",
    "                action = [action]\n",
    "            next_state, reward, terminal = env1.step(action)\n",
    "            reward_sum += reward[0]\n",
    "            agent.memory.add_to_buffer((state.reshape(1,15,10), next_state.reshape(1,15,10), action, [reward], [terminal]))\n",
    "            state = next_state\n",
    "            #env1.render()\n",
    "            if terminal:\n",
    "                reward_sums_list.append(reward_sum)\n",
    "                reward_sum = 0\n",
    "                break\n",
    "        if episode !=0:\n",
    "            agent.update(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-24a7b14692df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdqn_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-41f51dc3b189>\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(agent)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-79391e048420>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, update_rate)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mterminals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterminals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_Q_Network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-79391e048420>\u001b[0m in \u001b[0;36mupdate_Q_Network\u001b[0;34m(self, state, next_state, action, reward, terminal)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_Q_Network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mqsa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Q(s,a) (Value for selecting action given current Q-policy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mqsa_next_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mqsa_next_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqsa_next_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# AGAIN is this fair????\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mnot_terminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterminal\u001b[0m \u001b[0;31m# <- If it is not terminal (negation) then we will apply additional discount value:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2bb513b27510>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_agent(dqn_agent) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(8)\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('no. of episodes')\n",
    "ax1.set_ylabel('Total reward', color=color)\n",
    "ax1.scatter(range(0,len(reward_sums_list)), reward_sums_list, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('epsilon value', color=color)\n",
    "ax2.plot(range(0,len(epsilon_history)), epsilon_history, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max reward sum received: \", np.max(reward_sums_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"MSE error per update (of batch updates)\")\n",
    "plt.plot(dqn_agent.network_loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learnt Policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(agent):\n",
    "    max_time_steps = 5000\n",
    "    reward_sum = 0\n",
    "    state = env1.reset().flatten()\n",
    "    for i in range(max_time_steps):\n",
    "        chosen_actions = []\n",
    "        action = agent.policy_action(torch.from_numpy(state.flatten()).float())\n",
    "        next_state, reward, terminal = env1.step([action])\n",
    "        next_state = next_state.flatten() \n",
    "        reward_sum += reward[0]\n",
    "        state = next_state\n",
    "        env1.render()\n",
    "        if terminal:\n",
    "            time.sleep(0.2)\n",
    "            print(\"Reward sum is: \", reward_sum)\n",
    "            state = env1.reset().flatten()\n",
    "            reward_sum = 0\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent(dqn_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpratability test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env1.reset()\n",
    "env1.step([0])\n",
    "state = torch.from_numpy(env1.step([0])[0].flatten()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent.qnet(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalisation test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env1_mod1 = MultiPathBoardEnv(obstacles, starts, [(7,4)], grid_size=grid_size, agents_n=n_agents) \n",
    "reward_sums_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env1_mod1.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent():\n",
    "    N = 2\n",
    "    max_time_steps = 1000\n",
    "    for episode in range(N):\n",
    "        reward_sum = 0\n",
    "        state = env1_mod1.reset().flatten()\n",
    "        for i in range(max_time_steps):\n",
    "            chosen_actions = []\n",
    "            action = dqn_agent.policy_action(torch.from_numpy(state.flatten()).float())\n",
    "            next_state, reward, terminal = env1_mod1.step([action])\n",
    "            next_state = next_state.flatten() \n",
    "            reward_sum += reward[0]\n",
    "            state = next_state\n",
    "            env1_mod1.render()\n",
    "            if terminal:\n",
    "                time.sleep(0.2)\n",
    "                reward_sums_list.append(reward_sum)\n",
    "                state = env1_mod1.reset().flatten()\n",
    "                reward_sum = 0\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_sums_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env1.step([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = []\n",
    "for i in range(0,15):\n",
    "    test_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = set(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = random.sample(test_set,4)\n",
    "test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.difference(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = list(range(0,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            for i in range(self.grid_size[0]):\n",
    "                for j in range(self.grid_size[1]):\n",
    "                    coord_set.add((i,j)) # Do a pythonic list comprehension :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i,j) for i in range(15) for j in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1%2 is not 1:\n",
    "    print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
