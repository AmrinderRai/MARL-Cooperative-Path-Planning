{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patheffects as pe\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connector():\n",
    "    NORTH = 0\n",
    "    NORTHEAST = 1\n",
    "    EAST = 2\n",
    "    SOUTHEAST = 3  \n",
    "    SOUTH = 4 \n",
    "    SOUTHWEST = 5 \n",
    "    WEST = 6 \n",
    "    NORTHWEST = 7 \n",
    "    # Should the agent be able to stay still in one place too ??? Or move each time. \n",
    "\n",
    "    def __init__(self, start_coord, idx): \n",
    "        self.id = idx \n",
    "        self.trail = self.id\n",
    "        self.head = self.id + 0.1 # <<< WILL THIS WORK IF IT IS AN INT TYPE?? \n",
    "        self.goal = self.id + 0.2 \n",
    "        self.connection = [start_coord] # Will build... hopefully until Final is the end_coord\n",
    "\n",
    "    def get_head(self): # no _ as the Board might need it for rendering. \n",
    "        return self.connection[-1]\n",
    "    \n",
    "    def move(self, direction):\n",
    "        # Environment should always give available actions as 0 to 7 :) \n",
    "        self.connection.append(self._step(direction))\n",
    "\n",
    "    def pop_head(self):\n",
    "        self.connection.pop()\n",
    "\n",
    "    def _step(self, direction):\n",
    "        current_pos = self.get_head()\n",
    "        # ** IS THERE A SMARTER WAY OF DOING THIS??? MODULO OR SOMETHING?!... \n",
    "        if direction == self.NORTH:\n",
    "            return (current_pos[0]-1, current_pos[1])\n",
    "\n",
    "        elif direction == self.NORTHEAST:\n",
    "            return (current_pos[0]-1, current_pos[1]+1)\n",
    "\n",
    "        elif direction == self.EAST:\n",
    "            return (current_pos[0], current_pos[1]+1)\n",
    "\n",
    "        elif direction == self.SOUTHEAST:\n",
    "            return (current_pos[0]+1, current_pos[1]+1)\n",
    "\n",
    "        elif direction == self.SOUTH:\n",
    "            return (current_pos[0]+1, current_pos[1])        \n",
    "\n",
    "        elif direction == self.SOUTHWEST:\n",
    "            return (current_pos[0]+1, current_pos[1]-1)\n",
    "\n",
    "        elif direction == self.WEST:\n",
    "            return (current_pos[0], current_pos[1]-1)\n",
    "\n",
    "        elif direction == self.NORTHWEST:\n",
    "            return (current_pos[0]-1, current_pos[1]-1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Board(): \n",
    "    EMPTY_CELL  = 0\n",
    "    OBSTACLE_CELL = -1\n",
    "\n",
    "    def __init__(self, board_size=[30,20]): \n",
    "        self.height = board_size[0]\n",
    "        self.width = board_size[1]\n",
    "        self.grid = np.zeros((self.height, self.width),dtype=float)\n",
    "        self.grid[:,:] = self.EMPTY_CELL\n",
    "        self.available_squares = self.width * self.height\n",
    "        self.obstacles = [] \n",
    "        #self.connectors = # Generate list of n connectors << when turning this into a MARL problem.  \n",
    "\n",
    "    def init_obstacles(self):\n",
    "        # REDUNDANT... WE ARE CURRENTLY INITTING IN THE LOGIC_CONTROLLER CLASS :) \n",
    "        return None # PERHAPS THIS METHOD SHOULD BE IN LOGIC_CONTROLLER\n",
    "\n",
    "    def place_obstacle(self, coord):\n",
    "        self.cover_cell(coord, self.OBSTACLE_CELL)\n",
    "\n",
    "    def off_grid(self, coord): # Where coord is off the grid or not.. \n",
    "        return coord[0]<0 or coord[0]>=self.height or coord[1]<0 or coord[1]>=self.width\n",
    "\n",
    "    def obstacle_collision(self, coord):\n",
    "        return self.grid[coord] == self.OBSTACLE_CELL # Might need to use np.equal or something... \n",
    "\n",
    "    def check_failure(self, coord):\n",
    "        return self.off_grid(coord) or self.obstacle_collision(coord)\n",
    "\n",
    "    def cover_cell(self, coord, obj):\n",
    "        self.grid[coord] = obj \n",
    "\n",
    "    def cell_val(self, coord):\n",
    "        return self.grid[coord]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiPathBoardEnv():\n",
    "\n",
    "    def __init__(self, obstacles, starts, goals, grid_size=[20,15], agents_n=1):\n",
    "        self.grid_size = grid_size\n",
    "        self.obstacles = obstacles \n",
    "        self.connectors = []\n",
    "        self.n_agents = agents_n\n",
    "        self.action_space = [0, 1, 2, 3, 4, 5, 6, 7] # <- Explicit? Good practise? \n",
    "        # Assert that both these list sizes == n_agents after: \n",
    "        self.starts = starts\n",
    "        self.goals = goals \n",
    "        self.episode_ended = False \n",
    "        self._init_board()\n",
    "\n",
    "    def _init_board(self):\n",
    "        self.board = Board(board_size=self.grid_size)  # < TO DO \n",
    "        for obstacle in self.obstacles: \n",
    "            self.board.place_obstacle(obstacle) # << Actually the coord as argument :) \n",
    "        for i in range(0,self.n_agents): \n",
    "            idx = i+1 # Since 0 is reserved for empty cell \n",
    "            # Aslo initialise every connector object with the below info... \n",
    "            connector = Connector(self.starts[i], idx) # < TO DO \n",
    "            self.connectors.append(connector)\n",
    "            self.board.cover_cell(self.starts[i], connector.head) # TO FIX CONCAT INT WITH CHAR\n",
    "            self.board.cover_cell(self.goals[i],  connector.goal)  \n",
    "        #print(\"Second connector is: \", self.connectors[1])\n",
    "\n",
    "    def step(self, moves): \n",
    "        # Assert agent == moves \n",
    "        rewards = []\n",
    "        for i, move in enumerate(moves): \n",
    "            reward = self._move_connector(i, move)\n",
    "            rewards.append(reward)\n",
    "\n",
    "        return self.board.grid.copy(), rewards, self.episode_ended\n",
    "\n",
    "    def reset(self):\n",
    "        self.connectors = []\n",
    "        self._init_board()\n",
    "        self.episode_ended = False\n",
    "        return self.board.grid.copy()\n",
    "\n",
    "    def render(self):\n",
    "        # Turn interactive mode on.\n",
    "        plt.ion()\n",
    "        plt.figure(figsize=(20,10)) \n",
    "        ax = plt.gca()\n",
    "        ax.clear()\n",
    "        clear_output(wait = True)\n",
    "\n",
    "        # Prepare the environment plot\n",
    "        env_plot = self.board.grid.copy()\n",
    "        env_plot = env_plot.astype(int)\n",
    "        colors = ['grey', 'white', 'pink', 'red', 'green', 'blue', 'yellow', 'purple', 'purple']\n",
    "        cmap = ListedColormap(colors[:self.n_agents+2])\n",
    "        ax.matshow(env_plot, cmap=cmap) # < How to choose specific colours?\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def _move_connector(self, idx, direction):\n",
    "        connector = self.connectors[idx] \n",
    "        self.board.grid[connector.get_head()] = connector.trail # < get_trail to be implemented\n",
    "        reward = -1 \n",
    "        connector.move(direction)\n",
    "        #print(\"Connector connection is {head} and the goal is {goal} \".format(head=self.board.cell_val(connector.get_head()), goal=connector.goal))\n",
    "        if self.board.check_failure(connector.get_head()): # Off the grid \n",
    "            #print(\"Off the Grid !!!\")\n",
    "            reward -=10\n",
    "            self.episode_ended = True\n",
    "            connector.pop_head()\n",
    "        elif self.board.cell_val(connector.get_head())==connector.goal:\n",
    "            #print(\"Reached goal!!!\")\n",
    "            self.board.cover_cell(connector.get_head(),connector.head)\n",
    "            self.episode_ended = True # << TO DO: Modify for MARL ... Only that connector is done!!! \n",
    "            reward += 20 \n",
    "        elif self.board.cell_val(connector.get_head())!=self.board.EMPTY_CELL: # Collision with obstacle (including other connectors)\n",
    "            #print(\"Collision occured !!!\")\n",
    "            reward -=10  \n",
    "            self.episode_ended = True # IS IT A GOOD IDEA TO DO THIS?! DOES THIS CONTRIBUTE TO THE SPARSE FEEDBACK (HINDERED LEARNING)??? :'( \n",
    "        else:\n",
    "            self.board.cover_cell(connector.get_head(),connector.head)\n",
    "\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_agents = 1 \n",
    "grid_size = [8,5]\n",
    "obstacles = [(7,3), (4,2)]\n",
    "starts = [(6,4)]\n",
    "goals = [(2,1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env1 = MultiPathBoardEnv(obstacles, starts, goals, grid_size=grid_size, agents_n=n_agents) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAJCCAYAAAAP5kZhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAQqElEQVR4nO3c34tmB33H8e+32QR/FqEZiiSha0FCg2AiQ7AElEZbEhW96UUCCi3C3miJIIheLfkHxF5IYVFrQatYf4CI1QY0imCjkxjFuAppiJhqmwkixgqG6LcXO7LZZH48i/Psmc/s6wVD5tnnMPlwkn1zOHue7ZkpALL80dIDALh44g0QSLwBAok3QCDxBggk3gCBjmW8u/u27v5Rdz/c3e9des+Suvsj3f14d39/6S1HQXdf191f7e6z3f1Qd9+19KYldffzuvtb3f3dnfNx99KbjoLuvqK7v9PdX1h6y16OXby7+4qq+mBV3V5VN1TVnd19w7KrFvXRqrpt6RFHyNNV9e6Z+YuqenVVveMy///jN1V168y8sqpurKrbuvvVC286Cu6qqrNLj9jPsYt3Vd1cVQ/PzCMz81RVfbKq3rLwpsXMzNer6udL7zgqZuZnM/PAzvdP1rnfoNcsu2o5c86vdl5eufN1WX9yr7uvrao3VtWHlt6yn+MY72uq6ifPeP1YXca/Odlbd5+sqpuq6r5llyxr5xbBg1X1eFXdMzOX9fmoqg9U1Xuq6ndLD9nPcYx37/Jrl/WVBM/V3S+qqs9U1btm5pdL71nSzPx2Zm6sqmur6ubufsXSm5bS3W+qqsdn5v6ltxzkOMb7saq67hmvr62qny60hSOou6+sc+H++Mx8duk9R8XM/KKq7q3L+89IbqmqN3f3o3Xuluut3f2xZSft7jjG+9tV9fLufll3X1VVd1TV5xfexBHR3V1VH66qszPz/qX3LK27N7r7JTvfP7+qXl9VP1x21XJm5n0zc+3MnKxz7fjKzLx14Vm7Onbxnpmnq+qdVfXlOveHUZ+amYeWXbWc7v5EVX2zqq7v7se6++1Lb1rYLVX1tjp3RfXgztcblh61oJdW1Ve7+3t17sLnnpk5so/HcV77K2EB8hy7K2+Ay4F4AwQSb4BA4g0QSLwBAh3reHf3qaU3HCXOx3nOxYWcjwslnI9jHe+qOvL/AS4x5+M85+JCzseFjvz5OO7xBjiW1vIhnauvvnpOnjx56D/3Ym1vb9fGxsbSM44M5+M85+JCzseFjsr5ePTRR+uJJ57Y7S/bqxPr+BeePHmytra21vGjAS4bm5ube77ntglAIPEGCCTeAIHEGyCQeAMEEm+AQOINEEi8AQKJN0Ag8QYIJN4AgcQbIJB4AwQSb4BA4g0QSLwBAok3QCDxBggk3gCBxBsgkHgDBBJvgEDiDRBIvAECiTdAIPEGCCTeAIHEGyDQSvHu7tu6+0fd/XB3v3fdowDY34Hx7u4rquqDVXV7Vd1QVXd29w3rHgbA3la58r65qh6emUdm5qmq+mRVvWW9swDYzyrxvqaqfvKM14/t/BoAC1kl3r3Lr81zDuo+1d1b3b21vb39hy8DYE+rxPuxqrruGa+vraqfPvugmTkzM5szs7mxsXFY+wDYxSrx/nZVvby7X9bdV1XVHVX1+fXOAmA/Jw46YGae7u53VtWXq+qKqvrIzDy09mUA7OnAeFdVzcwXq+qLa94CwIp8whIgkHgDBBJvgEDiDRBIvAECiTdAIPEGCCTeAIHEGyCQeAMEEm+AQOINEEi8AQKJN0Ag8QYIJN4AgcQbIJB4AwQSb4BA4g0QSLwBAok3QCDxBggk3gCBxBsgkHgDBBJvgEDiDRBIvAECiTdAIPEGCCTeAIHEGyCQeAMEEm+AQOINEEi8AQKJN0Ag8QYIJN4AgcQbIJB4AwQSb4BA4g0QSLwBAok3QCDxBggk3gCBxBsgkHgDBBJvgEDiDRBIvAECiTdAIPEGCCTeAIHEGyCQeAMEEm+AQAfGu7s/0t2Pd/f3L8UgAA62ypX3R6vqtjXvAOAiHBjvmfl6Vf38EmwBYEXueQMEOrR4d/ep7t7q7q3t7e3D+rEA7OLQ4j0zZ2Zmc2Y2NzY2DuvHArALt00AAq3yqOAnquqbVXV9dz/W3W9f/ywA9nPioANm5s5LMQSA1bltAhBIvAECiTdAIPEGCCTeAIHEGyCQeAMEEm+AQOINEEi8AQKJN0Ag8QYIJN4AgcQbIJB4AwQSb4BA4g0QSLwBAok3QCDxBggk3gCBxBsgkHgDBBJvgEDiDRBIvAECiTdAIPEGCHRi6QGXha9tLb3gaHnt5tILIJ4rb4BA4g0QSLwBAok3QCDxBggk3gCBxBsgkHgDBBJvgEDiDRBIvAECiTdAIPEGCCTeAIHEGyCQeAMEEm+AQOINEEi8AQKJN0Ag8QYIJN4AgcQbIJB4AwQSb4BA4g0QSLwBAok3QCDxBggk3gCBDox3d1/X3V/t7rPd/VB333UphgGwtxMrHPN0Vb17Zh7o7hdX1f3dfc/M/GDN2wDYw4FX3jPzs5l5YOf7J6vqbFVds+5hAOztou55d/fJqrqpqu5bxxgAVrNyvLv7RVX1map618z8cpf3T3X3VndvbW9vH+ZGAJ5lpXh395V1Ltwfn5nP7nbMzJyZmc2Z2dzY2DjMjQA8yypPm3RVfbiqzs7M+9c/CYCDrHLlfUtVva2qbu3uB3e+3rDmXQDs48BHBWfmG1XVl2ALACvyCUuAQOINEEi8AQKJN0Ag8QYIJN4AgcQbIJB4AwQSb4BA4g0QSLwBAok3QCDxBggk3gCBxBsgkHgDBBJvgEDiDRBIvAECiTdAIPEGCCTeAIHEGyCQeAMEEm+AQOINEEi8AQKJN0CgE0sPuCy8dnPpBcAx48obIJB4AwQSb4BA4g0QSLwBAok3QCDxBggk3gCBxBsgkHgDBBJvgEDiDRBIvAECiTdAIPEGCCTeAIHEGyCQeAMEEm+AQOINEEi8AQKJN0Ag8QYIJN4AgcQbIJB4AwQSb4BA4g0QSLwBAok3QKAD493dz+vub3X3d7v7oe6++1IMA2BvJ1Y45jdVdevM/Kq7r6yqb3T3v8/Mf655GwB7ODDeMzNV9audl1fufM06RwGwv5XueXf3Fd39YFU9XlX3zMx9650FwH5WivfM/HZmbqyqa6vq5u5+xbOP6e5T3b3V3Vvb29uHvROAZ7iop01m5hdVdW9V3bbLe2dmZnNmNjc2Ng5pHgC7WeVpk43ufsnO98+vqtdX1Q/XPQyAva3ytMlLq+pfuvuKOhf7T83MF9Y7C4D9rPK0yfeq6qZLsAWAFfmEJUAg8QYIJN4AgcQbIJB4AwQSb4BA4g0QSLwBAok3QCDxBggk3gCBxBsgkHgDBBJvgEDiDRBIvAECiTdAIPEGCCTeAIHEGyCQeAMEEm+AQOINEEi8AQKJN0Ag8QYIJN4AgcQbINCJpQdw+bn77ruXnnCknD59eukJBHLlDRBIvAECiTdAIPEGCCTeAIHEGyCQeAMEEm+AQOINEEi8AQKJN0Ag8QYIJN4AgcQbIJB4AwQSb4BA4g0QSLwBAok3QCDxBggk3gCBxBsgkHgDBBJvgEDiDRBIvAECiTdAIPEGCCTeAIHEGyDQyvHu7iu6+zvd/YV1DgLgYBdz5X1XVZ1d1xAAVrdSvLv72qp6Y1V9aL1zAFjFqlfeH6iq91TV7/Y6oLtPdfdWd29tb28fyjgAdndgvLv7TVX1+Mzcv99xM3NmZjZnZnNjY+PQBgLwXKtced9SVW/u7ker6pNVdWt3f2ytqwDY14Hxnpn3zcy1M3Oyqu6oqq/MzFvXvgyAPXnOGyDQiYs5eGburap717IEgJW58gYIJN4AgcQbIJB4AwQSb4BA4g0QSLwBAok3QCDxBggk3gCBxBsgkHgDBBJvgEDiDRBIvAECiTdAIPEGCCTeAIHEGyCQeAMEEm+AQOINEEi8AQKJN0Ag8QYIJN4AgcQbIJB4AwQ6sfQALj+nT59eegLEc+UNEEi8AQKJN0Ag8QYIJN4AgcQbIJB4AwQSb4BA4g0QSLwBAok3QCDxBggk3gCBxBsgkHgDBBJvgEDiDRBIvAECiTdAIPEGCCTeAIHEGyCQeAMEEm+AQOINEEi8AQKJN0Ag8QYIJN4AgcQbINCJVQ7q7ker6smq+m1VPT0zm+scBcD+Vor3jr+amSfWtgSAlbltAhBo1XhPVf1Hd9/f3ad2O6C7T3X3VndvbW9vH95CAJ5j1XjfMjOvqqrbq+od3f2aZx8wM2dmZnNmNjc2Ng51JAAXWineM/PTnX8+XlWfq6qb1zkKgP0dGO/ufmF3v/j331fV31TV99c9DIC9rfK0yZ9W1ee6+/fH/+vMfGmtqwDY14HxnplHquqVl2ALACvyqCBAIPEGCCTeAIHEGyCQeAMEEm+AQOINEEi8AQKJN0Ag8QYIJN4AgcQbIJB4AwQSb4BA4g0QSLwBAok3QCDxBggk3gCBxBsgkHgDBBJvgEDiDRBIvAECiTdAIPEGCCTeAIHEGyDQiaUHAFzga1tLLzg6nvz1nm+58gYIJN4AgcQbIJB4AwQSb4BA4g0QSLwBAok3QCDxBggk3gCBxBsgkHgDBBJvgEDiDRBIvAECiTdAIPEGCCTeAIHEGyCQeAMEEm+AQOINEEi8AQKJN0Ag8QYIJN4AgcQbIJB4AwQSb4BA4g0QaKV4d/dLuvvT3f3D7j7b3X+57mEA7O3Eisf9Y1V9aWb+truvqqoXrHETAAc4MN7d/cdV9Zqq+ruqqpl5qqqeWu8sAPazym2TP6+q7ar65+7+Tnd/qLtf+OyDuvtUd29199b29vahDwXgvFXifaKqXlVV/zQzN1XV/1XVe5990MycmZnNmdnc2Ng45JkAPNMq8X6sqh6bmft2Xn+6zsUcgIUcGO+Z+Z+q+kl3X7/zS6+rqh+sdRUA+1r1aZN/qKqP7zxp8khV/f36JgFwkJXiPTMPVtXmmrcAsCKfsAQIJN4AgcQbIJB4AwQSb4BA4g0QSLwBAok3QCDxBggk3gCBxBsgkHgDBBJvgEDiDRBIvAECiTdAIPEGCCTeAIHEGyCQeAMEEm+AQOINEEi8AQKJN0Ag8QYIJN4AgcQbIJB4AwQ6sfQAuNzdfffdS084Uk6fPr30hKPjxS/Y8y1X3gCBxBsgkHgDBBJvgEDiDRBIvAECiTdAIPEGCCTeAIHEGyCQeAMEEm+AQOINEEi8AQKJN0Ag8QYIJN4AgcQbIJB4AwQSb4BA4g0QSLwBAok3QCDxBggk3gCBxBsgkHgDBBJvgEDiDRBIvAECHRjv7r6+ux98xtcvu/tdl2IcALs7cdABM/Ojqrqxqqq7r6iq/66qz615FwD7uNjbJq+rqv+amR+vYwwAq7nYeN9RVZ/Y7Y3uPtXdW929tb29/YcvA2BPK8e7u6+qqjdX1b/t9v7MnJmZzZnZ3NjYOKx9AOziYq68b6+qB2bmf9c1BoDVXEy876w9bpkAcGmtFO/ufkFV/XVVfXa9cwBYxYGPClZVzcyvq+pP1rwFgBX5hCVAIPEGCCTeAIHEGyCQeAMEEm+AQOINEEi8AQKJN0Ag8QYIJN4AgcQbIJB4AwQSb4BA4g0QSLwBAok3QCDxBggk3gCBxBsgkHgDBBJvgEDiDRBIvAECiTdAIPEGCCTeAIHEGyBQz8zh/9Du7ar68aH/4It3dVU9sfSII8T5OM+5uJDzcaGjcj7+bGY2dntjLfE+Krp7a2Y2l95xVDgf5zkXF3I+LpRwPtw2AQgk3gCBjnu8zyw94IhxPs5zLi7kfFzoyJ+PY33PG+C4Ou5X3gDHkngDBBJvgEDiDRBIvAEC/T9ix5KQjGPa4gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env1.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env 1 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reply Memory\n",
    "Batches of transitions are decorrelated (due to random sampling from past experience). \n",
    "Stabilizes and improved the DQN training procedure!\n",
    "\n",
    "-- Note: We may not ever use this explicitly defined object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** From Bath RL PyTorch tutorial :) *** \n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self):\n",
    "        self.buffer = []\n",
    "\n",
    "    def add_to_buffer(self, data):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        self.buffer.append(data)\n",
    "        \n",
    "    def sample_minibatch(self,minibatch_length):\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        terminals = []\n",
    "        for i in range(minibatch_length):\n",
    "            random_int = np.random.randint(0, len(self.buffer)-1) \n",
    "            transition = self.buffer[random_int]\n",
    "            states.append(transition[0])\n",
    "            next_states.append(transition[1])\n",
    "            actions.append(transition[2])\n",
    "            rewards.append(transition[3])\n",
    "            terminals.append(transition[4])\n",
    "        return torch.Tensor(states), torch.Tensor(next_states), torch.Tensor(actions), torch.Tensor(rewards), torch.Tensor(terminals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Now implement the right DQN network :) \n",
    "# understand batchnorm & get the dims right!... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN network (MLP only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Network(nn.Module):\n",
    "    \"\"\"For environments of shape (1,8,5) unrolled to (40,)\"\"\"\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.number_of_actions = action_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_dim, 24)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(24, 20)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.fc3 = nn.Linear(20, self.number_of_actions)\n",
    "        \n",
    "\n",
    "    def forward(self, state):\n",
    "        out = self.fc1(state)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = grid_size[0]*grid_size[1]\n",
    "action_dim = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.qnet = Q_Network(state_dim, action_dim) # Each DQN agent has it's own Q policy net :) \n",
    "        self.qnet_optim = torch.optim.Adam(self.qnet.parameters(), lr=0.001) # Optimiser to optimize our qnet parameters with learning rate 0.01\n",
    "        self.discount_factor = 0.99 \n",
    "        self.MSELoss_function = nn.MSELoss() # Takes input (required_grad=True) and output, then we can backprop with respect to this cost function to calcualte loss :) \n",
    "        self.memory = ReplayBuffer()\n",
    "        self.network_loss_history = []\n",
    "        \n",
    "    def epsilon_greedy_action(self, state, epsilon):\n",
    "        if np.random.uniform(0, 1) < epsilon: # <- EDIT to your style :)\n",
    "            return random.sample(env1.action_space,1)[0] # random sample \n",
    "        else: \n",
    "            network_output_to_numpy = self.qnet(state).data.numpy()\n",
    "            max_value = np.max(network_output_to_numpy)\n",
    "            max_indices = np.nonzero(network_output_to_numpy == max_value)[0]\n",
    "            policy_action = np.random.choice(max_indices) # Choose random of the max (To allow fair sampling at the beginning)       \n",
    "            return policy_action  \n",
    "    \n",
    "    def policy_action(self, state):\n",
    "        network_output_to_numpy = self.qnet(state).data.numpy()\n",
    "        max_value = np.max(network_output_to_numpy)\n",
    "        max_indices = np.nonzero(network_output_to_numpy == max_value)[0]\n",
    "        policy_action = np.random.choice(max_indices) # Choose random of the max (To allow fair sampling at the beginning)       \n",
    "        return policy_action  \n",
    "    \n",
    "    # Update Q Network based on our observation: \n",
    "    # TIP: Look at the Bellman equation :) \n",
    "    def update_Q_Network(self, state, next_state, action, reward, terminal):\n",
    "        qsa = torch.gather(self.qnet(state), dim=1, index=action.long()) # Q(s,a) (Value for selecting action given current Q-policy)\n",
    "        qsa_next_actions = self.qnet(next_state)\n",
    "        qsa_next_action, _ = torch.max(qsa_next_actions, dim=1, keepdim=True) # AGAIN is this fair???? \n",
    "        not_terminal = 1-terminal # <- If it is not terminal (negation) then we will apply additional discount value: \n",
    "        qsa_next_target = reward + not_terminal * self.discount_factor * qsa_next_action\n",
    "        q_network_loss = self.MSELoss_function(qsa, qsa_next_target.detach()) # ** Why detatch qsa_next_target so that no gradients pass it?\n",
    "        self.network_loss_history.append(q_network_loss.double())\n",
    "        self.qnet_optim.zero_grad() # Restard gradients for optimisation \n",
    "        q_network_loss.backward() # <- Calculate gradient... ** But what were we tracking??** \n",
    "        self.qnet_optim.step()  # Take a step according the gradient calculated above??** 🤔\n",
    "        \n",
    "    def update(self, update_rate):\n",
    "        for i in range(update_rate):\n",
    "            states, next_states, actions, rewards, terminals = self.memory.sample_minibatch(80)\n",
    "            states = torch.Tensor(states)\n",
    "            next_states = torch.Tensor(next_states)\n",
    "            actions = torch.Tensor(actions)\n",
    "            rewards = torch.Tensor(rewards)\n",
    "            terminals = torch.Tensor(terminals)\n",
    "            self.update_Q_Network(states, next_states, actions, rewards, terminals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init... \n",
    "dqn_agent = DQNAgent(state_dim, action_dim)\n",
    "reward_sums_list = []\n",
    "epsilon_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent):\n",
    "    N = 20000\n",
    "    max_time_steps = 1000\n",
    "    epsilon = 0.9\n",
    "    decay = 0.999\n",
    "    min_epsilon = 0.2\n",
    "    for episode in range(N):\n",
    "        reward_sum = 0\n",
    "        state = env1.reset().flatten()\n",
    "        for i in range(max_time_steps):\n",
    "            chosen_actions = []\n",
    "            epsilon = max(min_epsilon, epsilon*decay)\n",
    "            epsilon_history.append(epsilon)\n",
    "            action = agent.epsilon_greedy_action(torch.from_numpy(state.flatten()).float(), epsilon)\n",
    "            if type(action) is not list:\n",
    "                action = [action]\n",
    "            next_state, reward, terminal = env1.step(action)\n",
    "            next_state = next_state.flatten() \n",
    "            reward_sum += reward[0]\n",
    "            agent.memory.add_to_buffer((state, next_state, action, [reward], [terminal]))\n",
    "            state = next_state\n",
    "            #env1.render()\n",
    "            if terminal:\n",
    "                time.sleep(0.2)\n",
    "                reward_sums_list.append(reward_sum)\n",
    "                state = env1.reset().flatten()\n",
    "                reward_sum = 0\n",
    "                break\n",
    "        if episode !=0:\n",
    "            agent.update(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amrinderrai/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([80, 80, 1])) that is different to the input size (torch.Size([80, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-f78bdeac98c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdqn_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-813c4959ba9b>\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(agent)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m#env1.render()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0mreward_sums_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_agent(dqn_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(8)\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('no. of episodes')\n",
    "ax1.set_ylabel('Total reward', color=color)\n",
    "ax1.scatter(range(0,len(reward_sums_list)), reward_sums_list, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('epsilon value', color=color)\n",
    "ax2.plot(range(0,len(epsilon_history)), epsilon_history, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "plt.plot(epsilon_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_loss_history = dqn_agent.network_loss_history\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.scatter(range(0,len(net_loss_history)), net_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dqn_agent.network_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"MSE error per update (of batch updates)\")\n",
    "plt.plot(dqn_agent.network_loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(agent):\n",
    "    max_time_steps = 1000\n",
    "    reward_sum = 0\n",
    "    state = env1.reset().flatten()\n",
    "    for i in range(max_time_steps):\n",
    "        chosen_actions = []\n",
    "        action = agent.policy_action(torch.from_numpy(state.flatten()).float())\n",
    "        next_state, reward, terminal = env1.step([action])\n",
    "        next_state = next_state.flatten() \n",
    "        reward_sum += reward[0]\n",
    "        state = next_state\n",
    "        env1.render()\n",
    "        if terminal:\n",
    "            time.sleep(0.2)\n",
    "            print(\"Reward sum is: \", reward_sum)\n",
    "            state = env1.reset().flatten()\n",
    "            reward_sum = 0\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent(dqn_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env1_mod1 = MultiPathBoardEnv(obstacles, starts, [(7,4)], grid_size=grid_size, agents_n=n_agents) \n",
    "reward_sums_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env1_mod1.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent():\n",
    "    N = 2\n",
    "    max_time_steps = 1000\n",
    "    for episode in range(N):\n",
    "        reward_sum = 0\n",
    "        state = env1_mod1.reset().flatten()\n",
    "        for i in range(max_time_steps):\n",
    "            chosen_actions = []\n",
    "            action = dqn_agent.policy_action(torch.from_numpy(state.flatten()).float())\n",
    "            next_state, reward, terminal = env1_mod1.step([action])\n",
    "            next_state = next_state.flatten() \n",
    "            reward_sum += reward[0]\n",
    "            state = next_state\n",
    "            env1_mod1.render()\n",
    "            if terminal:\n",
    "                time.sleep(0.2)\n",
    "                reward_sums_list.append(reward_sum)\n",
    "                state = env1_mod1.reset().flatten()\n",
    "                reward_sum = 0\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_sums_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
