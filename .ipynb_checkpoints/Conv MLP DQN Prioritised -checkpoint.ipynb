{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcb_envs import MultiPathGridEnv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from dqn_agent import DQNAgent\n",
    "from agent_trainer import QLearner\n",
    "from IPython.display import display, clear_output\n",
    "import csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_agents = 1\n",
    "grid_size = [15,10]\n",
    "obstacles = [(3,3),(6,2), (6,3), (9,4), (9,3), (10,4), (8,9), (8,8)]\n",
    "starts = [(13,8)]\n",
    "goals = [(1,1)] # orig: (2,1) \n",
    "to_train = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env1 = MultiPathGridEnv(obstacles, starts, goals, grid_size=grid_size, obs_size=[40,40], agents_n=n_agents, train=to_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env1.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = grid_size[0]*grid_size[1]\n",
    "action_dim = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent = DQNAgent(action_dim, device)\n",
    "reward_sums_list = []\n",
    "epsilon_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent):\n",
    "    N = 100000\n",
    "    max_time_steps = 3000\n",
    "    epsilon = 0.4\n",
    "    decay = 0.9999\n",
    "    min_epsilon = 0.1\n",
    "    for episode in range(N):\n",
    "        reward_sum = 0\n",
    "        state = env1.reset()[0] # Indent into 0 for now and elsewhere :-) \n",
    "        epsilon = max(min_epsilon, epsilon*decay)\n",
    "        epsilon_history.append(epsilon)\n",
    "        for i in range(max_time_steps):\n",
    "            chosen_actions = []\n",
    "            t_state = (torch.from_numpy(state[0]).reshape(1,1,grid_size[0],grid_size[1]).float().to(device),torch.Tensor([state[1]]).to(device))\n",
    "            action = agent.epsilon_greedy_action(t_state, epsilon)\n",
    "            if type(action) is not list:\n",
    "                action = [action]\n",
    "            next_state, reward, terminal = env1.step(action)\n",
    "            next_state = next_state[0]\n",
    "            buff_next_state = (next_state[0].reshape(1,grid_size[0],grid_size[1]), next_state[1])\n",
    "            buff_state = (state[0].reshape(1,grid_size[0],grid_size[1]), state[1])\n",
    "            reward_sum += reward[0]\n",
    "            agent.memory.add_to_buffer((buff_state, buff_next_state, action, [reward], [terminal]))\n",
    "            state = next_state\n",
    "            #env1.render()\n",
    "            if terminal:\n",
    "                #clear_output(wait=False)\n",
    "                #display('Current episode: ' + str(episode))\n",
    "                reward_sums_list.append(reward_sum)\n",
    "                reward_sum = 0\n",
    "                break\n",
    "        if episode !=0:\n",
    "            agent.update(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Current episode: 231'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f78bdeac98c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdqn_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-09f5b7a58688>\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(agent)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/DataScience_Bath_cws/Thesis_dev/marl_board_env/dqn_agent.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, update_rate)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mterminals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterminals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_Q_Network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/DataScience_Bath_cws/Thesis_dev/marl_board_env/dqn_agent.py\u001b[0m in \u001b[0;36mupdate_Q_Network\u001b[0;34m(self, state, next_state, action, reward, terminal)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnet_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mq_network_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnet_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_agent(dqn_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it too... \n",
    "wtr = csv.writer(open ('learning_history.csv', 'w'), delimiter=',', lineterminator='\\n')\n",
    "for x in reward_sums_list : wtr.writerow ([x]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(8)\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('no. of episodes')\n",
    "ax1.set_ylabel('Total reward', color=color)\n",
    "cumsum_vec = np.cumsum(np.insert(reward_sums_list, 0, 0)) \n",
    "ma_vec = (cumsum_vec[3:] - cumsum_vec[:-3]) / 3 # <<-- 100 instead of 5\n",
    "ax1.plot(range(0,len(ma_vec)), ma_vec, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('epsilon value', color=color)\n",
    "cumsum_vec_e = np.cumsum(np.insert(epsilon_history, 0, 0)) \n",
    "ma_vec_e = (cumsum_vec_e[3:] - cumsum_vec_e[:-3]) / 3\n",
    "ax2.plot(range(0,len(ma_vec_e)), ma_vec_e, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max reward sum received: \", np.max(reward_sums_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learnt Policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(agent):\n",
    "    max_time_steps = 5000\n",
    "    reward_sum = 0\n",
    "    state = env1.reset()[0]\n",
    "    for i in range(max_time_steps):\n",
    "        t_state = (torch.from_numpy(state[0]).reshape(1,1,grid_size[0],grid_size[1]).float().to(device),torch.Tensor([state[1]]).to(device))\n",
    "        action = agent.policy_action(t_state)\n",
    "        next_state, reward, terminal = env1.step([action])\n",
    "        next_state = next_state[0]\n",
    "        reward_sum += reward[0]\n",
    "        state = next_state\n",
    "        env1.render()\n",
    "        if terminal:\n",
    "            state = env1.reset()[0]\n",
    "            reward_sum = 0\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_policy(dqn_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpratability test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env1.reset()\n",
    "env1.step([0])\n",
    "state = env1.step([0])[0][0]\n",
    "t_state = (torch.from_numpy(state[0]).reshape(1,1,grid_size[0],grid_size[1]).float().to(device),torch.Tensor([state[1]]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent.qnet(t_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dqn_agent.qnet.state_dict(), 'models/concat-mlp.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalisation test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env1_mod1 = MultiPathBoardEnv(obstacles, starts, [(7,4)], grid_size=grid_size, agents_n=n_agents) \n",
    "reward_sums_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env1_mod1.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent():\n",
    "    N = 2\n",
    "    max_time_steps = 1000\n",
    "    for episode in range(N):\n",
    "        reward_sum = 0\n",
    "        state = env1_mod1.reset().flatten()\n",
    "        for i in range(max_time_steps):\n",
    "            chosen_actions = []\n",
    "            action = dqn_agent.policy_action(torch.from_numpy(state.flatten()).float())\n",
    "            next_state, reward, terminal = env1_mod1.step([action])\n",
    "            next_state = next_state.flatten() \n",
    "            reward_sum += reward[0]\n",
    "            state = next_state\n",
    "            env1_mod1.render()\n",
    "            if terminal:\n",
    "                time.sleep(0.2)\n",
    "                reward_sums_list.append(reward_sum)\n",
    "                state = env1_mod1.reset().flatten()\n",
    "                reward_sum = 0\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERALISATION TEST TO BE COMPLETED!! :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
